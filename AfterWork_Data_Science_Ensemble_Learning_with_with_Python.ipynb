{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AfterWork Data Science: Ensemble Learning with with Python",
      "provenance": [],
      "collapsed_sections": [
        "BA_HMQbPHWQJ",
        "SPpfqZYzrKvs",
        "5YzXyzh8zmiM",
        "dKTC_Q4g52G1",
        "btf3OzyS52G8",
        "vjn6CpvU52G_",
        "eaPL6iDf7JIA",
        "tqWcOlZ97JIB",
        "rZdaSxPNmxW9",
        "PfdApR7Jmzm9",
        "1eBPjiM8m14s",
        "wBSYp3fVIupH",
        "uebT2koxr4V4",
        "B-eE6btO3KKM"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kundyyy/100-Days-Of-ML-Code/blob/master/AfterWork_Data_Science_Ensemble_Learning_with_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IR_TNDR4xihI",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"blue\">To use this notebook on Google Colaboratory, you will need to make a copy of it. Go to **File** > **Save a Copy in Drive**. You can then use the new copy that will appear in the new tab.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcVE8EgKxl2r",
        "colab_type": "text"
      },
      "source": [
        "# AfterWork Data Science: Ensemble Learning with with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA_HMQbPHWQJ",
        "colab_type": "text"
      },
      "source": [
        "## Importing the Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moay-lWZm7z1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will start by importing the necessary libraries\n",
        "# ---\n",
        "# \n",
        "import pandas as pd                # Pandas for data manipulation\n",
        "import numpy as np                 # Numpy for scientific computations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPpfqZYzrKvs",
        "colab_type": "text"
      },
      "source": [
        "## Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5YzXyzh8zmiM",
        "colab_type": "text"
      },
      "source": [
        "### <font color=\"blue\">Classification</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qyugRGHN52Gv",
        "colab": {}
      },
      "source": [
        "# Example \n",
        "# ---\n",
        "# Question: Will John, 40 years old with a salary of 2500 will buy a car?\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
        "# ---"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dKTC_Q4g52G1"
      },
      "source": [
        "##### Data Importation and Exploration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G0nv0nTv52G2",
        "colab": {}
      },
      "source": [
        "# Loading and previewing our dataset\n",
        "# ---\n",
        "# \n",
        "social_df = pd.read_csv('http://bit.ly/SocialNetworkAdsDataset')\n",
        "social_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5H9l0Kdt52G5",
        "colab": {}
      },
      "source": [
        "# Determining the size of our dataset\n",
        "# (records, columns)\n",
        "# ---\n",
        "# \n",
        "social_df.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "btf3OzyS52G8"
      },
      "source": [
        "##### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4rwVc4cJ52G8",
        "colab": {}
      },
      "source": [
        "# Normally during this stage we would perform quite a number of \n",
        "# procedures, but because our focus is only onlearning about the \n",
        "# different modeling algorithms, we will only perform once \n",
        "# essential step in ot dataset. We will perform encoding,\n",
        "# which will help us transform our categorical values in our \n",
        "# dataset into numerical values. \n",
        "# Lets see what happens when we encode the gender variable \n",
        "# to have only numerical values. \n",
        "# ---\n",
        "#\n",
        "social_df[\"Gender\"] = np.where(social_df[\"Gender\"].str.contains(\"Male\", \"Female\"), 1, 0)\n",
        "social_df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vjn6CpvU52G_"
      },
      "source": [
        "##### Data Modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybg1VYOGSXbr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Preparing our dataset for training\n",
        "# ---\n",
        "# We first divide our data into attributes and labels:\n",
        "# You can think of this as splitting our data set in dependent and independent variables \n",
        "# where Age and EstimatedSalary are the independent variables and Purchased are the dependent/label variable.\n",
        "# ---\n",
        "# \n",
        "X = social_df.iloc[:, [1, 2 ,3]].values  # Independent/predictor variables\n",
        "y = social_df.iloc[:, 4].values          # Dependent/label variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFyZcCeVScEp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the dataset into a training set and test set\n",
        "# ---\n",
        "# We will split our dataset into training data and test data. \n",
        "# Training data will be used to train our logistic model and test data will be used to validate our model\n",
        "# Because weâ€™ll use sklearn to split our data, we will import train_test_split from sklearn.model_selection\n",
        "# ---\n",
        "# \n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgn73UwVS9hf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature Scaling / Normalisation\n",
        "# ---\n",
        "# We then perform feature scaling / normalisation to scale our data between 0 and 1 so as to get better accuracy.\n",
        "# Here, scaling is important because there is a huge difference between Age and EstimatedSalary.\n",
        "# In addition, this would also reduce redundacy in our dataset. \n",
        "# ---\n",
        "# \n",
        "\n",
        "# We import our scaler from sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# We make an instance sc_X of the object StandardScaler.\n",
        "# You can think of making an instance as making a copy.\n",
        "sc_X = StandardScaler()\n",
        "\n",
        "# We then fit and transform X_train and X_test\n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cDNlsiGTaRF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# In this example, because we will be comparing how \n",
        "# the different classification models will perform\n",
        "# ---\n",
        "#\n",
        "from sklearn.linear_model import LogisticRegression # Logistic Regression Classifier\n",
        "from sklearn.tree import DecisionTreeClassifier     # Decision Tree Classifier\n",
        "from sklearn.svm import SVC                         # SVM Classifier\n",
        "from sklearn.naive_bayes import GaussianNB          # Naive Bayes Classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier  # KNN Classifier\n",
        "\n",
        "# We will also use our ensemble classifiers\n",
        "from sklearn.ensemble import BaggingClassifier           # Bagging Meta-Estimator Classifier\n",
        "from sklearn.ensemble import RandomForestClassifier      # RandomForest Classifier \n",
        "from sklearn.ensemble import AdaBoostClassifier          # AdaBoost Classifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier  # AdaBoost GradientBoostingClassifier\n",
        "import xgboost as xgb                                    # Importing the XGBoost librariy\n",
        "\n",
        "# Below, we make an instance classifier of the object LogisticRegression, \n",
        "# DecisionTreeClassifier, SVC, GaussianNB, KNeighborsClassifier, GaussianNB.\n",
        "# As we will get to see, each of the classifiers take different parameters.\n",
        "# ---\n",
        "# \n",
        "logistic_classifier = LogisticRegression(random_state = 0, solver='lbfgs')\n",
        "decision_classifier = DecisionTreeClassifier()\n",
        "svm_classifier = SVC()\n",
        "knn_classifier = KNeighborsClassifier(n_neighbors=5)\n",
        "naive_classifier = GaussianNB() \n",
        "\n",
        "# We start implementing ensemble methods by first using Bagging Classifiers\n",
        "# ---\n",
        "# Uncomment each classifier and run the respective code\n",
        "# ---\n",
        "# bagging_meta_classifier = BaggingClassifier(tree.DecisionTreeClassifier())\n",
        "# random_forest_classifier = RandomForestClassifier()\n",
        "\n",
        "# Boosting Classifiers\n",
        "# ---\n",
        "# ada_boost_classifier = AdaBoostClassifier()\n",
        "# gbm_classifier = GradientBoostingClassifier() \n",
        "# xg_boost_classifier = xgb.XGBClassifier() \n",
        "\n",
        "# Now using these classifiers to fit our data, X_train and y_train.\n",
        "# By fitting we mean we train our classifiers based on the train dataset.\n",
        "# ---\n",
        "# Upon running this cell, we should have classifiers that can predict \n",
        "# whether a person will buy a car or not.\n",
        "# ---\n",
        "# Don't worry about the output, we get GaussianNB because our Naive Bayes classifier\n",
        "# is the last one to be built.\n",
        "# ---\n",
        "#\n",
        "logistic_classifier.fit(X_train, y_train)\n",
        "decision_classifier.fit(X_train, y_train)\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "naive_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Bagging Classifiers\n",
        "# ---\n",
        "# bagging_meta_classifier.fit(X_train, y_train)\n",
        "# random_forest_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Boosting Classifiers\n",
        "# ---\n",
        "# ada_boost_classifier.fit(X_train, y_train)\n",
        "# gbm_classifier.fit(X_train, y_train)\n",
        "# xg_boost_classifier.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAx-7zU7Tyrt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We now predict the test set results. \n",
        "# This will help us determine whether our classifiers made the correct predictions.\n",
        "# ---\n",
        "# No expected output here.\n",
        "# ---\n",
        "logistic_y_prediction = logistic_classifier.predict(X_test) \n",
        "decision_y_prediction = decision_classifier.predict(X_test) \n",
        "svm_y_prediction = svm_classifier.predict(X_test) \n",
        "knn_y_prediction = knn_classifier.predict(X_test) \n",
        "naive_y_prediction = naive_classifier.predict(X_test) \n",
        "\n",
        "# Bagging Classifiers\n",
        "# ---\n",
        "# bagging_y_classifier = bagging_meta_classifier.predict(X_test) \n",
        "# random_forest_y_classifier = random_forest_classifier.predict(X_test) \n",
        "\n",
        "# Boosting Classifiers\n",
        "# ---\n",
        "# ada_boost_y_classifier = ada_boost_classifier.predict(X_test)\n",
        "# gbm_y_classifier = gbm_classifier.predict(X_test)\n",
        "# xg_boost_y_classifier = xg_boost_classifier.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG_q-tVaUFxy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We then import evaluation metrics to determine the accuracy of classifiers\n",
        "# ---\n",
        "# \n",
        "from sklearn.metrics import classification_report, accuracy_score \n",
        "\n",
        "# The accuracy score - is the simplest way to evaluate \n",
        "# However, we note not for a highly imbalance dataset. \n",
        "# By imbalanced we mean that our original dataset would\n",
        "# need to have an equal no's of 1 and 0's\n",
        "# ---\n",
        "print(\"Logistic Regression Classifier\", accuracy_score(logistic_y_prediction, y_test))\n",
        "print(\"Decision Trees Classifier\", accuracy_score(decision_y_prediction, y_test))\n",
        "print(\"SVN Classifier\", accuracy_score(svm_y_prediction, y_test))\n",
        "print(\"KNN Classifier\", accuracy_score(knn_y_prediction, y_test))\n",
        "print(\"Naive Bayes Classifier\", accuracy_score(naive_y_prediction, y_test))\n",
        "\n",
        "# Bagging Classifiers\n",
        "# ---\n",
        "# print(\"Bagging Classifier\", accuracy_score(bagging_y_classifier, y_test))\n",
        "# print(\"Random Forest Classifier\", accuracy_score(random_forest_y_classifier, y_test))\n",
        "\n",
        "# Boosting Classifiers\n",
        "# ---\n",
        "# print(\"Ada Boost Classifier\", accuracy_score(ada_boost_y_classifier, y_test))\n",
        "# print(\"GBM Classifier\", accuracy_score(gbm_y_classifier, y_test))\n",
        "# print(\"XGBoost Classifier\", accuracy_score(xg_boost_y_classifier, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4wgNdhhrdVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We now print the classification report, \n",
        "# which is more reliable for a highly imbalanced dataset. \n",
        "# We use the precision values which give us accuracy values.\n",
        "# \n",
        "# ---\n",
        "# The precision will be \"how many are correctly classified among that class\".\n",
        "# The recall means \"how many of this class you find over the whole number of element of this class\".\n",
        "# The f1-score is the harmonic mean between precision & recall.\n",
        "# The support is the number of occurence of the given class in your dataset.\n",
        "# ---\n",
        "# \n",
        "print('Logistic classifier:')\n",
        "print(classification_report(y_test, logistic_y_prediction))\n",
        "\n",
        "print('Decision Tree classifier:')\n",
        "print(classification_report(y_test, decision_y_prediction))\n",
        "\n",
        "print('SVM Classifier:')\n",
        "print(classification_report(y_test, svm_y_prediction))\n",
        "\n",
        "print('KNN Classifier:')\n",
        "print(classification_report(y_test, knn_y_prediction))\n",
        "\n",
        "print('Naive Bayes Classifier:')\n",
        "print(classification_report(y_test, naive_y_prediction)) \n",
        "\n",
        "# Bagging Classifiers\n",
        "# ---\n",
        "# print('Bagging Meta Classifier:')\n",
        "# print(classification_report(y_test, bagging_y_classifier)) \n",
        "\n",
        "# print('Random Forest Classifier:')\n",
        "# print(classification_report(y_test, random_forest_y_classifier)) \n",
        "\n",
        "\n",
        "# Boosting Classifiers\n",
        "# ---\n",
        "# print('Ada Boost Classifier:')\n",
        "# print(classification_report(y_test, ada_boost_y_classifier)) \n",
        "\n",
        "# print('GBM Classifier:')\n",
        "# print(classification_report(y_test, gbm_y_classifier)) \n",
        "\n",
        "# print('XGBoost Classifier:')\n",
        "# print(classification_report(y_test, xg_boost_y_classifier)) \n",
        "\n",
        "# Remember, we can then further perform model opmization techiniques i.e. \n",
        "# data cleaning, feature engineering, checking for model assumptions, etc. \n",
        "# to further get the best classifier. "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnqqUOET9QLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Answering our question\n",
        "# ---\n",
        "# We then make a new prediction & compare results.\n",
        "# Note that we would only use the best optimized classifier for this case.\n",
        "# ---\n",
        "# Predict whether John, 60 years old with a salary of 2500 will buy a car or not?\n",
        "# ---\n",
        "# Dataset limitation: This is not a practical dataset, thus dataset will lack essential features/variables.\n",
        "# In a real case scenario, we would work with may kinds of datasets that require transformation\n",
        "# i.e. data cleaning, feature engineering, etc.\n",
        "# ---\n",
        "#\n",
        "new_case = [[1,\t60, 1500]]\n",
        "\n",
        "print(\"Logistic Regression Classifier\", logistic_classifier.predict(new_case))\n",
        "print(\"Decision Tree Classifier\",decision_classifier.predict(new_case))\n",
        "print(\"SVM Classifier\", svm_classifier.predict(new_case))\n",
        "print(\"KNN Classifier\", knn_classifier.predict(new_case))\n",
        "print(\"Naive Bayes Classifier\", naive_classifier.predict(new_case))\n",
        "\n",
        "# Bagging Classifiers\n",
        "# ---\n",
        "# print(\"Bagging Meta Classifier\", bagging_meta_classifier.predict(new_case))\n",
        "# print(\"Random Forest Classifier\", random_forest_classifier.predict(new_case))\n",
        "\n",
        "# Boosting Classifiers\n",
        "# ---\n",
        "# print(\"Ada Boosting Classifier\", ada_boost_classifier.predict(new_case))\n",
        "# print(\"GBM Classifier\", gbm_classifier.predict(new_case))\n",
        "# print(\"XGBoost Classifier\", xg_boost_classifier.predict(new_case)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eaPL6iDf7JIA"
      },
      "source": [
        "### <font color=\"blue\">Regression</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tqWcOlZ97JIB"
      },
      "source": [
        "##### <font color=\"blue\">Example</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MXpXZxEW7JIC",
        "colab": {}
      },
      "source": [
        "# Example\n",
        "# --- \n",
        "# Questions: Create a decision tree regression model using the following dataset.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/FishDatasetClean\n",
        "# NB: This dataset is clean version of the one \n",
        "# we used in the multiple regression example above.\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "# "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZdaSxPNmxW9",
        "colab_type": "text"
      },
      "source": [
        "##### Step 1. Loading our Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjnbZeaGmioQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading our data\n",
        "# ---\n",
        "# \n",
        "df = pd.read_csv('http://bit.ly/FishDatasetClean')\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "477dYK8zNrPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Describing our dataset\n",
        "# ---\n",
        "# \n",
        "df.describe()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfdApR7Jmzm9",
        "colab_type": "text"
      },
      "source": [
        "##### Step 2, 3, 4: Checking, Cleaning, Exploratory Analysis and have already been performed on our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eBPjiM8m14s",
        "colab_type": "text"
      },
      "source": [
        "##### Step 5. Implementation and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rprclJHCmiQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's now split our dataset\n",
        "# ---\n",
        "# \n",
        "# Firstly, importing our train_test_split function\n",
        "# ---\n",
        "#\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df[['Length1', 'Length2', 'Length3', 'Height', 'Width']]\n",
        "y = df['Weight']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM2qYhp2miKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets now train our regressors\n",
        "# ---\n",
        "#  \n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor \n",
        "from sklearn.ensemble import BaggingRegressor \n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "# Creating our regressors, We'll just use the decision tree regressor this time\n",
        "# ---\n",
        "# \n",
        "regressor = DecisionTreeRegressor()\n",
        "\n",
        "# Then creating our ensemble regressors\n",
        "# ---\n",
        "\n",
        "# Bagging Regressors\n",
        "# ---\n",
        "# bagging_est_regressor = BaggingRegressor()\n",
        "# random_forest_regressor = RandomForestRegressor()\n",
        "\n",
        "# Boosting Regressors\n",
        "# ---\n",
        "# ada_boost_regressor = AdaBoostRegressor()\n",
        "# gbm_regressor = GradientBoostingRegressor()\n",
        "# xgboost_regressor = xgb.XGBRegressor(objective ='reg:squarederror') # It requires us to specify the objective function\n",
        "\n",
        "# Fitting our data to our regressors \n",
        "# ---\n",
        "# Decision Tree Regressor\n",
        "# regressor.fit(X_train, y_train)\n",
        "\n",
        "# Bagging Regressors\n",
        "# ---\n",
        "# bagging_est_regressor.fit(X_train, y_train)\n",
        "# random_forest_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Boosting Regressors\n",
        "# ---\n",
        "# ada_boost_regressor.fit(X_train, y_train)\n",
        "# gbm_regressor.fit(X_train, y_train)\n",
        "# xgboost_regressor.fit(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9FqCDpRpahT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making predictions using our models\n",
        "# ---\n",
        "#  \n",
        "y_pred = regressor.predict(X_test)\n",
        "\n",
        "# Bagging Regressors\n",
        "# ---\n",
        "# bag_est_y_pred = bagging_est_regressor.predict(X_test)\n",
        "# random_forest_y_pred = random_forest_regressor.predict(X_test)\n",
        "\n",
        "# Boosting Regressors\n",
        "# ---\n",
        "# ada_boost_y_pred = ada_boost_regressor.predict(X_test)\n",
        "# gbm_y_pred = gbm_regressor.predict(X_test)\n",
        "# xgboost_y_pred = xgboost_regressor.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n51lYFVs9tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Next, we compare actual output values for X_test with the predicted values\n",
        "# ---\n",
        "#\n",
        "df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryp4KnEUP2yP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We make predictings for the random forest regressor\n",
        "# ---\n",
        "# Next, we compare actual output values for X_test with the predicted values\n",
        "# ---\n",
        "# random_forest_df = pd.DataFrame({'Actual': y_test, 'Predicted': random_forest_y_pred})\n",
        "# random_forest_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNWV3e5RIBks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We make predictions for the adaboost regressor\n",
        "# ---\n",
        "# Next, we compare actual output values for X_test with the predicted values\n",
        "# ---\n",
        "# ada_boost_df = pd.DataFrame({'Actual': y_test, 'Predicted': ada_boost_y_pred})\n",
        "# ada_boost_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7jzgfuWRui6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We make predictions for the gbm regressor\n",
        "# ---\n",
        "# Next, we compare actual output values for X_test with the predicted values\n",
        "# ---\n",
        "# gbm_df = pd.DataFrame({'Actual': y_test, 'Predicted': gbm_y_pred})\n",
        "# gbm_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvtwjGM-VpcP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We also predict for the XGboost regressor\n",
        "# ---\n",
        "# Next, we compare actual output values for X_test with the predicted values\n",
        "# ---\n",
        "# xgboost_df = pd.DataFrame({'Actual': y_test, 'Predicted': xgboost_y_pred})\n",
        "# xgboost_df.head(5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tuOEMtNDmh9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finally, we evaluate the models\n",
        "# ---  \n",
        "# NB: The closer the RMSE is to 0, the better the model.\n",
        "#  \n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "print('Decision Tree - Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, y_pred)))\n",
        "\n",
        "# Bagging Regressors\n",
        "# ---\n",
        "# print('Bagging Estimator - Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, bag_est_y_pred)))\n",
        "# print('Random Forest - Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, random_forest_y_pred)))\n",
        "\n",
        "# Boosting Regressors\n",
        "# ---\n",
        "# print('Ada Boost - Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, ada_boost_y_pred)))\n",
        "# print('GBM - Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, gbm_y_pred)))\n",
        "# print('XGBoost - Root Mean Squared Error:', np.sqrt(mean_squared_error(y_test, xgboost_y_pred)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBSYp3fVIupH",
        "colab_type": "text"
      },
      "source": [
        "##<font color=\"green\">Challenges</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uebT2koxr4V4",
        "colab_type": "text"
      },
      "source": [
        "###<font color=\"green\">Challenge 1</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVcDuh88sFLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Question: A cancer medical reasearch institution would like to make predictions on two different \n",
        "# cancer types benign and malignant. Build a model to predict the breast cancer type \n",
        "# (0 = benign or 1 = malignant) given the following dataset. In addition, make a prediction.\n",
        "# NB: Remember to record your observations and also implement ensemble techniques with the goal of improving accuracy.\n",
        "# Make a recommendation on the best model to use for this problem.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/BreastCancersDataset\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-eE6btO3KKM",
        "colab_type": "text"
      },
      "source": [
        "###<font color=\"green\">Challenge 2</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9b-Q2CMI2wg1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Challenge \n",
        "# ---\n",
        "# Predict the price of cars comparing your models using the following dataset. \n",
        "# Make a recommendation on the best model to use for this problem.\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/CarPriceDataset\n",
        "# ---\n",
        "# OUR CODE GOES BELOW\n",
        "#"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}