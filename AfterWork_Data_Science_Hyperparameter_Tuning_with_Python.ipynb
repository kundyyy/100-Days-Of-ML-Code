{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AfterWork Data Science: Hyperparameter Tuning with Python",
      "provenance": [],
      "collapsed_sections": [
        "I4acj-OTOP82",
        "2jTFOxfaOd14",
        "0CrlFuI-VjsD",
        "QaKj_EYnVnJa",
        "m4f-HCCzcsFn",
        "S6xAx-PccsFq",
        "mIk0U5Aqfhbz",
        "X-RiKkKFOrVb",
        "A9y1H556gVW_",
        "W6ndQUsSizcy"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kundyyy/100-Days-Of-ML-Code/blob/master/AfterWork_Data_Science_Hyperparameter_Tuning_with_Python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxYyRzuYN6u7",
        "colab_type": "text"
      },
      "source": [
        "<font color=\"blue\">To use this notebook on Google Colaboratory, you will need to make a copy of it. Go to **File** > **Save a Copy in Drive**. You can then use the new copy that will appear in the new tab.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ABs5Pr5OOdM",
        "colab_type": "text"
      },
      "source": [
        "# AfterWork Data Science: Hyperparameter Tuning with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4acj-OTOP82",
        "colab_type": "text"
      },
      "source": [
        "### Pre-requisites"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpvueFt9N2Wr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will start by running this cell which will import the necessary libraries\n",
        "# ---\n",
        "# \n",
        "import pandas as pd                # Pandas for data manipulation\n",
        "import numpy as np                 # Numpy for scientific computations\n",
        "import matplotlib.pyplot as plt    # Matplotlib for visualisation - We might not use it but just incase you decide to \n",
        "%matplotlib inline                 "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jTFOxfaOd14",
        "colab_type": "text"
      },
      "source": [
        "## 1. Manual Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CrlFuI-VjsD",
        "colab_type": "text"
      },
      "source": [
        "### Example "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQtTRKfhiMyR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example \n",
        "# ---\n",
        "# Question: Will John, 40 years old with a salary of 2500 will buy a car?\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rr4jsDQ7UgDk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Steps 1\n",
        "# ---\n",
        "# Loading our dataset \n",
        "social_df = pd.read_csv('http://bit.ly/SocialNetworkAdsDataset') \n",
        "\n",
        "# Data preparation: Encoding\n",
        "social_df[\"Gender\"] = np.where(social_df[\"Gender\"].str.contains(\"Male\", \"Female\"), 1, 0) \n",
        "\n",
        "# Defining our predictor and label variable\n",
        "X = social_df.iloc[:, [1, 2 ,3]].values  # Independent/predictor variables\n",
        "y = social_df.iloc[:, 4].values          # Dependent/label variable\n",
        "\n",
        "# Splitting our dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
        "\n",
        "\n",
        "# Performing scaling\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler() \n",
        "X_train = sc_X.fit_transform(X_train)\n",
        "X_test = sc_X.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAjFNtLnOgJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Steps 2\n",
        "# ---\n",
        "# Defining our classifier\n",
        "from sklearn.tree import DecisionTreeClassifier  \n",
        "\n",
        "# We will get to see the values of the Decision Tree classifier hyper parameters in the output below \n",
        "# The decision tree has a quite a number of hyperparameters that require fine-tuning in order \n",
        "# to get the best possible model that reduces the generalization error. \n",
        "# To explore other decision tree hyperparameters, we can explore the sckit-learn documentation \n",
        "# by following this link: https://bit.ly/3eu3XIh\n",
        "# ---\n",
        "# We will focus on two specific hyperparameters:\n",
        "# 1. Max depth: This is the maximum number of children nodes that can grow out from \n",
        "# the decision tree until the tree is cut off. \n",
        "# For example, if this is set to 3, then the tree will use three children nodes \n",
        "# and cut the tree off before it can grow any more. \n",
        "# 2. Min samples leaf: This is the minimum number of samples, or data points, \n",
        "# that are required to be present in the leaf node.\n",
        "# ---\n",
        "#\n",
        "decision_classifier = DecisionTreeClassifier()\n",
        "\n",
        "# Fitting our data\n",
        "decision_classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSqqtM59QxMw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Steps 3\n",
        "# ---\n",
        "# Making our predictions\n",
        "decision_y_prediction = decision_classifier.predict(X_test) \n",
        "\n",
        "# Determining the Accuracy\n",
        "from sklearn.metrics import accuracy_score \n",
        "print(accuracy_score(decision_y_prediction, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE20z-jvT69F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Repeating Steps 2\n",
        "# ---\n",
        "# Let's now perform hyper parameter tuning by setting \n",
        "# the hyperparameters max_depth = 2 and min_samples_leaf = 100\n",
        "# and get our output?\n",
        "# ---\n",
        "# \n",
        "decision_classifier = DecisionTreeClassifier(max_depth = 2, min_samples_leaf = 50)\n",
        "\n",
        "# Fitting our data\n",
        "decision_classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rygoolwDUzJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Repeating Steps 3\n",
        "# --- \n",
        "# Steps 3\n",
        "# ---\n",
        "# Making our predictions\n",
        "decision_y_prediction = decision_classifier.predict(X_test) \n",
        "\n",
        "# Determining the Accuracy\n",
        "from sklearn.metrics import accuracy_score \n",
        "print(accuracy_score(decision_y_prediction, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KQIlwybVZtn",
        "colab_type": "text"
      },
      "source": [
        "Can you get a better accuracy? By tuning the same hyperparameters or other parameters?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJD2CgDZXVNj",
        "colab_type": "text"
      },
      "source": [
        "To read more about hyper parameter tuning for decision trees, you can refer to this reading: [Link](https://towardsdatascience.com/how-to-tune-a-decision-tree-f03721801680)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaKj_EYnVnJa",
        "colab_type": "text"
      },
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsWOOvFmVvHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Challenge 1\n",
        "# ---\n",
        "# Using the given dataset above, create a logistic regression classifier \n",
        "# then tune its hyperparameters to get the best possible accuracy.\n",
        "# Make a comparisons of your with other fellows in your breakout rooms.\n",
        "# Hint: Use the following documentation to tune the hyper parameters.\n",
        "# Sckit-learn documentation: https://bit.ly/2YZR4iP\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m4f-HCCzcsFn"
      },
      "source": [
        "## 2. Grid Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S6xAx-PccsFq"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFnMcdWliR-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Example \n",
        "# ---\n",
        "# Question: Will John, 40 years old with a salary of 2500 will buy a car?\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jotViGxKb8Yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Steps 2\n",
        "# ---\n",
        "# Defining our classifier \n",
        "\n",
        "# We will get to see the values of the Decision Tree classifier hyper parameters in the output below \n",
        "# The decision tree has a quite a number of hyperparameters that require fine-tuning in order \n",
        "# to get the best possible model that reduces the generalization error. \n",
        "# To explore other decision tree hyperparameters, we can explore the sckit-learn documentation \n",
        "# by following this link: https://bit.ly/3eu3XIh\n",
        "# ---\n",
        "# Again we will focus on the same two specific hyperparameters:\n",
        "# 1. Max depth: This is the maximum number of children nodes that can grow out from \n",
        "# the decision tree until the tree is cut off. \n",
        "# For example, if this is set to 3, then the tree will use three children nodes \n",
        "# and cut the tree off before it can grow any more. \n",
        "# 2. Min samples leaf: This is the minimum number of samples, or data points, \n",
        "# that are required to be present in the leaf node.\n",
        "# ---\n",
        "#\n",
        "decision_classifier = DecisionTreeClassifier()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q03AxVzIZprI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 3: Hyperparameters: Getting Started with Grid Search\n",
        "# ---\n",
        "# We will continue from where we left off from the previous example,\n",
        "# We create a dictionary of all the parameters and their corresponding \n",
        "# set of values that you want to test for best performance. \n",
        "# The name of the dictionary items corresponds to the parameter name \n",
        "# and the value corresponds to the list of values for the parameter.\n",
        "# As shown grid_param dictionary with three parameters n_estimators, criterion, and bootstrap. \n",
        "# The parameter values that we want to try out are passed in the list. \n",
        "# For instance, in the above script we want to find which value \n",
        "# (out of 100, 300, 500, 800, and 1000) provides the highest accuracy. \n",
        "# Similarly, we want to find which value results in the \n",
        "# highest performance for the criterion parameter: \"gini\" or \"entropy\"? \n",
        "# The Grid Search algorithm basically tries all possible combinations \n",
        "# of parameter values and returns the combination with the highest accuracy. \n",
        "# For instance, in the above case the algorithm will check all combinations (5 x 5 = 25).\n",
        "# ---\n",
        "# \n",
        "grid_param = {\n",
        "    'max_depth': [2, 3, 4, 10, 15],\n",
        "    'min_samples_leaf': [10, 20, 30, 40, 50]\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FgzqZY71Z2Vv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 2: Instantiating GridSearchCV object\n",
        "# ---\n",
        "# Once the parameter dictionary is created, the next step \n",
        "# is to create an instance of the GridSearchCV class. \n",
        "# We need to pass values for the estimator parameter, \n",
        "# which basically is the algorithm that you want to execute. \n",
        "# The param_grid parameter takes the parameter dictionary \n",
        "# that we just created as parameter, the scoring parameter \n",
        "# takes the performance metrics, the cv parameter corresponds \n",
        "# to number of folds, which will set 5 in our case, and finally \n",
        "# the n_jobs parameter refers to the number of CPU's that we want to use for execution. \n",
        "# A value of -1 for n_jobs parameter means that use all available computing power.\n",
        "# You can refer to the GridSearchCV documentation \n",
        "# if you want to find out more: https://bit.ly/2Yr0qVC\n",
        "# ---\n",
        "# \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "gd_sr_cl = GridSearchCV(estimator = decision_classifier,\n",
        "                     param_grid = grid_param,\n",
        "                     scoring = 'accuracy',\n",
        "                     cv = 5,\n",
        "                     n_jobs =-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xurXa_ovZ5JE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 3: Calling the fit method\n",
        "# ---\n",
        "# Once the GridSearchCV class is initialized, we call the fit method of the class \n",
        "# and pass it the training and test set, as shown in the following code.\n",
        "# The method might take abit of some time to execute. \n",
        "# This is the drawback - GridSearchCV will go through all the intermediate \n",
        "# combinations of hyperparameters which makes grid search computationally very expensive.\n",
        "# ---\n",
        "# \n",
        "gd_sr_cl.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSjIyP6iZ7gM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 4: Checking the parameters that return the highest accuracy\n",
        "# --- \n",
        "# To do so, we print the sr.best_params_ attribute of the GridSearchCV object, as shown below:\n",
        "# ---\n",
        "# \n",
        "best_parameters = gd_sr_cl.best_params_\n",
        "print(best_parameters)\n",
        "\n",
        "# The result shows that the highest accuracy is achieved \n",
        "# when the n_estimators are 300, bootstrap is True and criterion is \"gini\". \n",
        "# It would be a good idea to add more number of estimators \n",
        "# and see if performance further increases since the highest \n",
        "# allowed value of n_estimators was chosen."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY8IpIK1Z9gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 5: Finding the obtained accuracy\n",
        "# ---\n",
        "# The last and final step of Grid Search algorithm is \n",
        "# to find the accuracy obtained using the best parameters. \n",
        "# Previously we had a mean accuracy of 64.22%.\n",
        "# To find the best accuracy achieved, we execute the following code:\n",
        "# ---\n",
        "# \n",
        "best_result = gd_sr_cl.best_score_\n",
        "print(best_result)\n",
        "\n",
        "# The accuracy achieved is: 0.6505 of 65.05% which is only slightly better than 64.22%. \n",
        "# To improve this further, it would be good to test values for other parameters \n",
        "# of Random Forest algorithm, such as max_features, max_depth, max_leaf_nodes, etc. \n",
        "# to see if the accuracy further improves or not."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NZOSpZBc7CU",
        "colab_type": "text"
      },
      "source": [
        "Can you get a better accuracy? By refering to the decision tree documentation, choosing additional approriate hyper-parameters and set the hyperparameter values to the grid search space in an effort to get a better accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIk0U5Aqfhbz",
        "colab_type": "text"
      },
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEMGtQlpfk8c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Challenge\n",
        "# ---\n",
        "# In this challenge, we still be required to use grid search while using \n",
        "# the logistic regression classifier we created earlier to get the best possible accuracy. \n",
        "# Hint: Use the following documentation to tune the hyperparameters.\n",
        "# Sckit-learn documentation: https://bit.ly/2YZR4iP\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-RiKkKFOrVb",
        "colab_type": "text"
      },
      "source": [
        "## 3. Random Search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "A9y1H556gVW_"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fPGxiGQFgVXB",
        "colab": {}
      },
      "source": [
        "# Example \n",
        "# ---\n",
        "# Question: Will John, 40 years old with a salary of 2500 will buy a car?\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
        "# ---\n",
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUTlhWCWglW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 1: Hyperparameters: Getting Started with Random Search\n",
        "# ---\n",
        "# Random search differs from grid search in that we longer \n",
        "# provide a discrete set of values to explore for each hyperparameter; rather, \n",
        "# we provide a statistical distribution for each hyperparameter \n",
        "# from which values may be randomly sampled.\n",
        "# We'll define a sampling distribution for each hyperparameter.\n",
        "# ---\n",
        "# \n",
        "\n",
        "# specify parameters and distributions to sample from\n",
        "from scipy.stats import randint as sp_randint\n",
        "param_dist = {\"max_depth\": [3, None], \n",
        "              \"min_samples_leaf\": sp_randint(1, 50)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyDyEIalgmiN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 2: Instantiating RandomizedSearchCV object \n",
        "# ---\n",
        "# Documentation: https://bit.ly/2V9Xhri\n",
        "# \n",
        "from sklearn.model_selection import RandomizedSearchCV \n",
        "random_sr = RandomizedSearchCV(decision_classifier, param_dist, cv = 5) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mVVunKJgrMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 3: Calling the fit method\n",
        "# ---\n",
        "#\n",
        "random_sr.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4v1u9vVguNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Step 4: Checking the parameters that return the highest accuracy\n",
        "# ---\n",
        "#\n",
        "best_parameters = random_sr.best_params_\n",
        "print(best_parameters)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzKzEqLxgvx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Finding the obtained accuracy\n",
        "# --\n",
        "# \n",
        "best_result = random_sr.best_score_\n",
        "print(best_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx4Qs-8rjUQu",
        "colab_type": "text"
      },
      "source": [
        "Can you get a better accuracy? By refering to the decision tree documentation, choosing additional approriate hyper-parameters and set the hyperparameter values to the random search space in an effort to get a better accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "W6ndQUsSizcy"
      },
      "source": [
        "### <font color=\"green\">Challenge</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "taJUBjUJizc0",
        "colab": {}
      },
      "source": [
        "# Challenge\n",
        "# ---\n",
        "# Again, we will also be required to use random search while using \n",
        "# the logistic regression classifier we created earlier to get the best possible accuracy. \n",
        "# Hint: Use the following documentation to tune the hyperparameters.\n",
        "# Sckit-learn documentation: https://bit.ly/2YZR4iP\n",
        "# ---\n",
        "# Dataset url = http://bit.ly/SocialNetworkAdsDataset\n",
        "# "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}